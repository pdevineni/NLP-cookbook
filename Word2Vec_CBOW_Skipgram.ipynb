{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "**Word Embeddings**: A word embedding represents words as vectors in a high-dimensional space, where similar words are closer to each other. \n",
    "\n",
    "Word embeddings enable algorithms to understand the semantic relationships between words and perform tasks like language translation, sentiment analysis, and text classification more effectively.\n",
    "\n",
    "In Word2Vec, word embeddings are created using a shallow neural network model trained on a large corpus of text. There are two main architectures for training word embeddings in Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram.\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**: In CBOW, the model is trained to predict the target word based on its context words within a certain window size. The input to the model is a context of surrounding words, and the output is the target word. The model learns to map words to vectors in such a way that similar words have similar vectors.\n",
    "\n",
    "\n",
    "2. **Skip-gram**: In Skip-gram, the model is trained to predict the context words given a target word. The input to the model is a single word, and the output is the probability distribution of context words within a certain window size. The model learns to generate word embeddings by maximizing the probability of predicting context words given the target word.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/skipgram_detailed.png\" alt=\"Skip-gram Architecture\" style=\"width: 600px;\"/>\n",
    "<figcaption align = \"center\"> Skip-gram architecture </figcaption>\n",
    "</center>\n",
    "\n",
    "\n",
    "During training, the neural network adjusts the word embeddings (vector representations) based on the errors in predicting the target or context words. The training process involves iterating through the corpus multiple times (epochs) to update the word embeddings until they capture meaningful semantic relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Aspect |\tCBOW  |\tSkip-gram |\n",
    "|---\t|---\t|---\t|\n",
    "| Architecture |\tPredicts a target word given its context  |\tPredicts context words given a target word |\n",
    "| Objective\t| Minimizes the loss of predicting a word |\tMinimizes the loss of predicting context |\n",
    "| Training Data\t| Requires a large corpus of text |\tRequires a large corpus of text | \n",
    "| Computational Efficiency\t| Typically trains faster than Skip-gram | Requires more training iterations and can be computationally expensive | \n",
    "| Model Size |\tSmaller model size compared to Skip-gram | Larger model size compared to CBOW | \n",
    "| Context Window Size |\tFixed window size for context words | Can handle varying context window sizes | \n",
    "| Word Representations | Dense word vectors | Dense word vectors | \n",
    "| Semantic Relationships | Loses some of the finer sematic details | Captures more fine-grained semantic relationships | \n",
    "| Performance on Rare Words | Tends to perform poorly on rare words | Tends to perform better on rare words | \n",
    "| Performance on Frequent Words | Tends to perform well on frequent words | Tends to perform worse on frequent words | \n",
    "| Use Cases\t| Good for applications with limited resources | Useful for a wider range of NLP tasks | \n",
    "| Typical Application | Pretrained word embeddings for small datasets | Pretrained word embeddings for large datasets | \n",
    "| Word2Vec Implementation |\tOften preferred for Word2Vec implementation\t| Also used for Word2Vec but with some modifications | \n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/cbow_skipgram_example.png\" alt=\"Example use of CBOW and Skip-gram\" style=\"width: 600px;\"/>\n",
    "<figcaption align = \"center\"> Example use of CBOW and Skip-gram </figcaption>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Model: \n",
    "\n",
    "- The CBOWModel class is a custom PyTorch Module\n",
    "- It consists of an embedding layer (nn.Embedding) and a linear layer (nn.Linear).\n",
    "- Embedding Layer: Converts the indices of context words into dense vectors. \n",
    "    - The nn.Embedding layer takes two parameters: vocabulary size and the dimensionality of the embeddings.\n",
    "    - Each unique word in the vocabulary will be associated with a unique embedding vector in this layer. \n",
    "    - The output of the embedding layer is averaged to a get a single vector that represents the context.\n",
    "    - When the model is trained, these vectors are updated in a way that words with similar meanings end up with similar vectors. This allows the model to generalize well to unseen data.\n",
    "- Linear Layer: a fully connected layer or a dense layer applies a linear transformation to the incoming data. \n",
    "    - It is defined by two parameters: the number of input features and the number of output features.\n",
    "    - In this case, embedding_dim is the number of input features and vocab_size is the number of output features. \n",
    "    - The vocab_size can be interpreted as the unnormalized log probabilities for each word in the vocabulary being the target word.\n",
    "    - The nn.Linear class automatically creates the weight and bias tensors, which are learned during training. \n",
    "                output = input.matmul(weight.t()) + bias\n",
    "\n",
    "- The torch.mean function computes the average of the embeddings of the context words. \n",
    "- The context vector is then passed through the linear layer to predict the target word.\n",
    "\n",
    "<center>\n",
    "<img src=\"images/cbow_detailed.png\" alt=\"CBOW Architecture\" style=\"width: 600px;\"/>\n",
    "<figcaption align = \"center\"> CBOW architecture </figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** ------ *** ------ * TRAINING * ------ *** ------ **\n",
      " \n",
      "Epoch: 0/100 Loss: 230.70034790039062\n",
      "Epoch: 10/100 Loss: 111.81954956054688\n",
      "Epoch: 20/100 Loss: 42.16696548461914\n",
      "Epoch: 30/100 Loss: 17.44841194152832\n",
      "Epoch: 40/100 Loss: 9.785517692565918\n",
      "Epoch: 50/100 Loss: 6.553323268890381\n",
      "Epoch: 60/100 Loss: 4.847179889678955\n",
      "Epoch: 70/100 Loss: 3.8093013763427734\n",
      "Epoch: 80/100 Loss: 3.117750644683838\n",
      "Epoch: 90/100 Loss: 2.6279261112213135\n",
      "\n",
      "** ------ *** ------ * TESTING * ------ *** ------ **\n",
      " \n",
      "Raw text: We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of rules called a program. People create programs to direct processes. In effect, we conjure the spirits of the computer with our spells.\n",
      "Context: ['People', 'create', 'to', 'direct']\n",
      "Prediction: programs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "EMDEDDING_DIM = 300\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = torch.tensor([word_to_ix[word]])\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "    \n",
    "\n",
    "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "print (\"\\n** ------ *** ------ * TRAINING * ------ *** ------ **\\n \")\n",
    "\n",
    "iterations = 100\n",
    "for epoch in range(iterations):\n",
    "    total_loss = 0\n",
    "\n",
    "    for context, target in data:\n",
    "        context_vector = make_context_vector(context, word_to_ix)  \n",
    "\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
    "    if epoch % 10 == 0:\n",
    "        print ('Epoch:', str(epoch)+\"/\"+str(iterations), 'Loss:', total_loss.item())\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# TESTING\n",
    "print (\"\\n** ------ *** ------ * TESTING * ------ *** ------ **\\n \")\n",
    "context = ['People','create','to', 'direct']\n",
    "context_vector = make_context_vector(context, word_to_ix)\n",
    "a = model(context_vector)\n",
    "\n",
    "#Print result\n",
    "print(f'Raw text: {\" \".join(raw_text)}')\n",
    "print(f'Context: {context}')\n",
    "print(f'Prediction: {ix_to_word[torch.argmax(a[0]).item()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Skip gram implementation using PyTorch \n",
    "\n",
    "https://github.com/lukysummer/SkipGram_with_NegativeSampling_Pytorch/blob/master/SkipGram_NegativeSampling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "1. https://jalammar.github.io/illustrated-word2vec/\n",
    "2. https://github.com/OlgaChernytska/word2vec-pytorch/tree/main\n",
    "3. https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
